{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Learning with PyMC\n",
    "\n",
    "This notebook demonstrates a Bayesian semi-supervised learning approach using PyMC with entropy minimization on unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "import pytensor.tensor as pt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=2000, n_features=2, n_informative=2, \n",
    "                           n_redundant=0, weights=[0.8, 0.2], flip_y=0.1, random_state=42)\n",
    "\n",
    "# Split: Labeled (small) vs Unlabeled (large)\n",
    "X_labeled, X_unlabeled, y_labeled, _ = train_test_split(X, y, train_size=100, random_state=42)\n",
    "\n",
    "print(f\"Labeled: {len(X_labeled)} | Unlabeled: {len(X_unlabeled)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Scalable PyMC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # --- Input Data Containers (Mutable for future mini-batching) ---\n",
    "    X_l = pm.Data(\"X_labeled\", X_labeled)\n",
    "    y_l = pm.Data(\"y_labeled\", y_labeled)\n",
    "    X_u = pm.Data(\"X_unlabeled\", X_unlabeled)\n",
    "\n",
    "    # --- Parameters ---\n",
    "    # Global calibration (applied to the weighted ensemble)\n",
    "    # We simplified: Weight -> Ensemble -> Calibrate\n",
    "    w = pm.Dirichlet(\"weights\", a=np.ones(2))\n",
    "    a = pm.HalfNormal(\"slope\", sigma=1.0)\n",
    "    b = pm.Normal(\"bias\", mu=0.0, sigma=1.0)\n",
    "\n",
    "    # --- Forward Pass ---\n",
    "    # 1. Ensemble Combination (Linear)\n",
    "    # Note: We combine logits first, then calibrate. \n",
    "    # This is numerically more stable than combining probabilities.\n",
    "    ens_logit_l = pm.math.dot(X_l, w)\n",
    "    ens_logit_u = pm.math.dot(X_u, w)\n",
    "    \n",
    "    # 2. Calibration (Platt Scaling)\n",
    "    p_labeled = pm.math.sigmoid(a * ens_logit_l + b)\n",
    "    p_unlabeled = pm.math.sigmoid(a * ens_logit_u + b)\n",
    "\n",
    "    # --- Likelihood (Labeled Data) ---\n",
    "    obs = pm.Bernoulli(\"y_obs\", p=p_labeled, observed=y_l)\n",
    "\n",
    "    # --- SSL: Entropy Minimization (The Critical Fix) ---\n",
    "    # We penalize high uncertainty (p near 0.5) on unlabeled data.\n",
    "    # This forces the decision boundary into low-density regions.\n",
    "    entropy = -(p_unlabeled * pt.log(p_unlabeled + 1e-6) + \n",
    "               (1 - p_unlabeled) * pt.log(1 - p_unlabeled + 1e-6))\n",
    "    \n",
    "    # \"lambda_ssl\" controls how much we trust unlabeled structure vs labeled data\n",
    "    lambda_ssl = 0.5 \n",
    "    pm.Potential(\"ssl_regularization\", -lambda_ssl * entropy.sum())\n",
    "\n",
    "    # --- Inference: ADVI for Scalability ---\n",
    "    print(\"Fitting with ADVI (Variational Inference)...\")\n",
    "    mean_field = pm.fit(method='advi', n=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Sampling & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw samples from the approximated posterior\n",
    "trace = mean_field.sample(1000)\n",
    "\n",
    "# Extract learned parameters directly from trace (no need for sample_posterior_predictive)\n",
    "print(\"\\nLearned Weights (Mean):\")\n",
    "print(trace.posterior[\"weights\"].mean(dim=[\"draw\", \"chain\"]).values)\n",
    "\n",
    "print(\"\\nLearned Slope (Mean):\")\n",
    "print(trace.posterior[\"slope\"].mean(dim=[\"draw\", \"chain\"]).values)\n",
    "\n",
    "print(\"\\nLearned Bias (Mean):\")\n",
    "print(trace.posterior[\"bias\"].mean(dim=[\"draw\", \"chain\"]).values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
